{"cells":[{"cell_type":"code","source":["from sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom __future__ import division\nimport numpy as np\n\n\nclass KNN(object):\n    \n    def __init__(self, neighbors):\n        #TODO\n        self.neighbors=neighbors\n      \n    def fit(self, X, y):\n        '''\n        X multidimensional\n        y unidimensional\n        '''\n        #TODO\n        self.X_train=X\n        self.y_train=y\n\n    def predict(self, X):\n        '''\n        X multidimensional\n        y unidimensional\n        '''\n        #TODO\n        distancias=[]\n        vecinos=[]\n        resultados=[]\n        for i in range(len(X)):\n            for j in range(len(self.X_train)):\n              distancia= np.sqrt(np.sum(np.square(self.X_train[j,:]-X[i,:])))\n              distancias.append([distancia,j])\n            distancias = sorted(distancias)\n            for k in range(self.neighbors):\n              index = distancias[k][1]\n              vecinos.append(self.y_train[index])\n            resultados.append([np.sum(vecinos)/self.neighbors])\n            del distancias[:]\n            del vecinos[:]\n        return resultados\n      \n    def score(self, X, y):\n        '''\n        X multidimensional\n        y unidimensional\n        '''\n        #TODO         \n        return (1/len(X))*np.sum(np.square(self.predict(X)-y))\n\nclass GradientDescent(object):\n    b = 0 #Theta0\n    m = 0 #Theta1\n    def __init__(self, alfa):\n        #TODO\n        self.alfa = alfa\n\n    def fit(self, x, y):\n        '''\n        x unidimensional\n        y unidimensional\n        '''\n        #TODO\n        # a = b + mx\n        #theta0 = b; theta1=m\n        for i in range(len(x)):\n          sumb=0\n          summ=0\n          for j in range(len(x)):\n            sumb = sumb+((self.b+self.m*x[j])-y[j])\n            summ = summ+(((self.b+self.m*x[j])-y[j])*y[j])\n          self.b = self.b - self.alfa*((1/len(x))*sumb) \n          self.m = self.m - self.alfa*((1/len(x))*summ)\n\n    def predict(self, x):\n        '''\n        x unidimensional\n        y unidimensional\n        '''\n        #TODO\n        resultado=[]\n        for i in range(len(x)):\n          resultado.append(self.b+self.m*x[i])\n        return resultado\n      \n\n    def score(self, x, y):\n        '''\n        x unidimensional\n        y unidimensional\n        '''\n        #TODO\n        return (1/len(x))*np.sum(np.square(self.predict(x)-y))\n\ndef CV_kNN(X, y, hyperparams):\n    '''\n    X multidimensional\n    y unidimensional\n    '''\n    #TODO   \n    bestScore = np.inf\n    bestNeighbor = 0\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)#Dividimos los datos en 10 pliegues, 9 para el entreiento 1 para el testeo\n   \n    for i in range(hyperparams):\n      knn = KNN(i)\n      knn.fit(X_train,Y_train)\n      score=knn.score(X_test,Y_test)\n      if(score<bestScore):\n        bestScore=score\n        bestNeighbor=i\n    bestKNN=KNN(bestNeighbor)\n    bestKNN.fit(X_train,Y_train)\n    return bestNeighbor,bestKNN\n\nif __name__ == \"__main__\":\n    np.random.seed(1)\n    iris = datasets.load_iris()\n    X=iris.data[:,2:3]\n    Y=iris.data[:,3:4]\n    # Crea data sets de entrenamiento y testeo\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n\n    #KNN\n    knn = KNN(5)\n    knn.fit(X_train,Y_train)\n    knnScore = knn.score(X_test,Y_test)\n    print('KNN, score: '+str(knnScore))\n    \n    #~Descenso de gradiente\n    gd = GradientDescent(0.05)\n    gd.fit(X_train, Y_train)\n    gdScore = SGD.score(X_test,Y_test)\n    print('Descenso de gradiente, score: '+str(gdScore))\n    \n    #K-nn regressor con CV\n    cvknn=CV_kNN(X, Y, 40)\n    print('ValidaciÃ³n cruzada, mejor parametro: '+str(cvknn[0]))\n"],"metadata":{},"outputs":[],"execution_count":1}],"metadata":{"name":"P1.JAVIER_GUILLAMON-MARINO_PEREZ","notebookId":912990108007647},"nbformat":4,"nbformat_minor":0}
